Creating a plagiarism checker program was more difficult than I expected. The concept was simple, or so I thought. I simply needed to search through two text files and find similar words. I quickly realized that the challenge was finding a way to do this efficiently.

To make it simpler, I defined a text section to be a consecutive group of words from a file, containing a word count equal to a specified search diameter. I also defined the similarity between two text sections to be the number of unique words common to both sections. The naive solution would be to loop through and compare all text sections from file 1 against all text sections from file 2, counting the number of words that are shared between both sections. If I had files containing 1020 words each and a search diameter of 20 words, I would need to loop at least a million times.

Luckily, I was able to optimize the program by focusing on words common to both files. I created a dictionary to instantly retrieve all the indices of a given word within a file. Then, to find the similarity between two text sections, I chose to compare a section from file 1 with a section from file 2 if they were centered around a common word. As a tradeoff for reducing the number of text comparisons, this strategy does miss out on some possible matches. This strategy also performs worse if the input files contain a lot of repeated words.
